<!DOCTYPE html>
<html lang="en">
    <!-- to run this: python3 -m http.server 8000 -->

    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>In browser agent demo</title>
    </head>

    <body style="margin: 3em;">
        <div id="output"></div>
        <script src="https://unpkg.com/modprompt@0.7.7/dist/mod.min.js"></script>
        <script type="module">
            import { WllamaProvider } from "https://unpkg.com/@locallm/browser@0.0.6/dist/main.js";

            const out = document.getElementById('output');
            const lm = WllamaProvider.init({
                onToken: (t) => { out.innerText = t },
            },
                // the static wasm files are loaded from the /esm/ url
                "/esm/"
            );
            const model = {
                name: "Qween 0.5b",
                url: "https://huggingface.co/Qwen/Qwen2-0.5B-Instruct-GGUF/resolve/main/qwen2-0_5b-instruct-q5_k_m.gguf",
                ctx: 32768,
            }

            const onModelLoading = (st) => {
                const msg = "Model downloading: " + st.percent + " %";
                console.log(msg);
                out.innerText = msg;
                if (st.percent == 100) {
                    out.innerText = "Loading model into memory ..."
                }
            }

            lm.loadBrowsermodel(model.name, model.url, model.ctx, onModelLoading).then(() => {
                out.innerText = "Ingesting prompt ...";
                const p = new $tpl.PromptTemplate("chatml")
                    .replaceSystem("You are an AI assistant")
                    .prompt("List the orbital periods of the planets of the solar system.")
                lm.infer(
                    p,
                    { temperature: 0, min_p: 0.05 }
                ).then((res) => {
                    console.log("Stats", res.stats)
                });
            });
        </script>
    </body>

</html>